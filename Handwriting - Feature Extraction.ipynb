{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRTKBsYGJSsI",
        "outputId": "f09e9cf7-6aaa-414c-b6c1-9c8b4f927db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/parkinson+disease+spiral+drawings+using+digitized+graphics+tablet.zip\n",
            "   creating: hw_dataset/\n",
            "   creating: hw_dataset/control/\n",
            "  inflating: hw_dataset/control/C_0001.txt  \n",
            "  inflating: hw_dataset/control/C_0002.txt  \n",
            "  inflating: hw_dataset/control/C_0003.txt  \n",
            "  inflating: hw_dataset/control/C_0004.txt  \n",
            "  inflating: hw_dataset/control/C_0005.txt  \n",
            "  inflating: hw_dataset/control/C_0006.txt  \n",
            "  inflating: hw_dataset/control/C_0007.txt  \n",
            "  inflating: hw_dataset/control/C_0008.txt  \n",
            "  inflating: hw_dataset/control/C_0009.txt  \n",
            "  inflating: hw_dataset/control/C_0010.txt  \n",
            "  inflating: hw_dataset/control/C_0011.txt  \n",
            "  inflating: hw_dataset/control/C_0012.txt  \n",
            "  inflating: hw_dataset/control/C_0013.txt  \n",
            "  inflating: hw_dataset/control/C_0014.txt  \n",
            "  inflating: hw_dataset/control/C_0015.txt  \n",
            "   creating: hw_dataset/parkinson/\n",
            "  inflating: hw_dataset/parkinson/P_02100001.txt  \n",
            "  inflating: hw_dataset/parkinson/P_02100002.txt  \n",
            "  inflating: hw_dataset/parkinson/P_05060003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_05060004.txt  \n",
            "  inflating: hw_dataset/parkinson/P_09100001.txt  \n",
            "  inflating: hw_dataset/parkinson/P_09100003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_09100005.txt  \n",
            "  inflating: hw_dataset/parkinson/P_11120003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_11120004.txt  \n",
            "  inflating: hw_dataset/parkinson/P_11120005.txt  \n",
            "  inflating: hw_dataset/parkinson/P_12060001.txt  \n",
            "  inflating: hw_dataset/parkinson/P_12060002.txt  \n",
            "  inflating: hw_dataset/parkinson/P_16100003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_16100004.txt  \n",
            "  inflating: hw_dataset/parkinson/P_23100002.txt  \n",
            "  inflating: hw_dataset/parkinson/P_23100003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_26060001.txt  \n",
            "  inflating: hw_dataset/parkinson/P_26060002.txt  \n",
            "  inflating: hw_dataset/parkinson/P_26060003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_26060006.txt  \n",
            "  inflating: hw_dataset/parkinson/P_26060007.txt  \n",
            "  inflating: hw_dataset/parkinson/P_27110001.txt  \n",
            "  inflating: hw_dataset/parkinson/P_27110003.txt  \n",
            "  inflating: hw_dataset/parkinson/P_30100001.txt  \n",
            "  inflating: hw_dataset/parkinson/P_30100002.txt  \n",
            "  inflating: hw_dataset/readme.txt   \n",
            "   creating: hw_drawings/\n",
            "   creating: hw_drawings/Dynamic Spiral Test/\n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d1.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d10.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d11.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d12.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d13.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d14.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d15.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d16.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d17.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d18.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d19.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d2.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d20.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d21.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d22.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d23.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d24.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d25.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d3.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d4.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d5.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d6.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d7.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d8.png  \n",
            "  inflating: hw_drawings/Dynamic Spiral Test/d9.png  \n",
            "   creating: hw_drawings/Static Spiral Test/\n",
            "  inflating: hw_drawings/Static Spiral Test/s1.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s10.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s11.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s12.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s13.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s14.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s15.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s16.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s17.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s18.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s19.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s2.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s20.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s21.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s22.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s23.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s24.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s25.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s3.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s4.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s5.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s6.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s7.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s8.png  \n",
            "  inflating: hw_drawings/Static Spiral Test/s9.png  \n",
            "   creating: new_dataset/\n",
            "   creating: new_dataset/parkinson/\n",
            "  inflating: new_dataset/parkinson/H_P000-0001.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0002.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0003.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0004.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0007.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0008.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0010.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0011.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0012.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0013.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0014.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0015.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0016.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0017.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0018.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0019.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0020.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0021.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0022.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0023.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0024.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0025.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0028.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0029.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0030.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0031.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0032.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0033.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0034.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0035.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0036.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0037.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0039.txt  \n",
            "  inflating: new_dataset/parkinson/H_P000-0040.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0041.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0042.txt  \n",
            "  inflating: new_dataset/parkinson/H_p000-0043.txt  \n",
            "  inflating: readme.txt              \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/parkinson+disease+spiral+drawings+using+digitized+graphics+tablet.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4XY2CfGVO6T",
        "outputId": "bccf7ba6-489b-470b-c244-4daaa6bf0a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data augmentation complete. Augmented images are saved in: /content/augmented/images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/hw_drawings\"  # Replace with the folder containing spiral images\n",
        "augmented_dir = \"/content/augmented/images\"  # Replace with the folder to save augmented images\n",
        "\n",
        "# Create ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Perform data augmentation\n",
        "if not os.path.exists(augmented_dir):\n",
        "    os.makedirs(augmented_dir)\n",
        "\n",
        "for filename in os.listdir(data_dir):\n",
        "    img_path = os.path.join(data_dir, filename)\n",
        "    # Check if it's a file before processing\n",
        "    if os.path.isfile(img_path):\n",
        "        img = load_img(img_path, target_size=(224, 224))  # Resize to 224x224\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = img_array.reshape((1,) + img_array.shape)\n",
        "\n",
        "        # Generate augmented images\n",
        "        i = 0\n",
        "        for batch in datagen.flow(img_array, batch_size=1, save_to_dir=augmented_dir, save_prefix='aug', save_format='jpg'):\n",
        "            i += 1\n",
        "            if i > 5:  # Generate 5 augmented images per input image\n",
        "                break\n",
        "\n",
        "print(\"Data augmentation complete. Augmented images are saved in:\", augmented_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvTDBs8tVuaV",
        "outputId": "973c3411-6eea-4d2e-c3e8-4bb99f5fa97b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 0 images.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import os\n",
        "\n",
        "# Preprocess images\n",
        "def preprocess_images(img_dir):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(img_dir):\n",
        "        img_path = os.path.join(img_dir, filename)\n",
        "        # Check if it's a file before processing\n",
        "        if os.path.isfile(img_path):  # Only process if it's a file\n",
        "            img = load_img(img_path, target_size=(224, 224), color_mode='grayscale')  # Convert to grayscale\n",
        "            img_array = img_to_array(img)\n",
        "            img_array = img_array / 255.0  # Normalize pixel values to [0, 1]\n",
        "            img_array = np.stack((img_array,) * 3, axis=-1)  # Convert grayscale to 3-channel (required for CNNs)\n",
        "            img_array = preprocess_input(img_array)  # Additional preprocessing for CNN\n",
        "            images.append(img_array)\n",
        "            filenames.append(filename)\n",
        "    return np.array(images), filenames\n",
        "\n",
        "# Example usage\n",
        "processed_images, filenames = preprocess_images(\"/content/augmented/images\")  # Corrected path\n",
        "print(f\"Processed {len(processed_images)} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-4WjmEXZkI",
        "outputId": "84f52b35-725f-4a23-a510-d289423ccbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data augmentation completed. Augmented images are saved in: /content/augmented1/images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/hw_drawings/Dynamic Spiral Test\"  # Replace with the folder containing spiral images\n",
        "augmented_dir = \"/content/augmented1/images\"  # Replace with the folder to save augmented images\n",
        "\n",
        "# Create the augmented directory if it does not exist\n",
        "if not os.path.exists(augmented_dir):\n",
        "    os.makedirs(augmented_dir)\n",
        "\n",
        "# ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,           # Random rotation within 20 degrees\n",
        "    width_shift_range=0.2,       # Random horizontal shift\n",
        "    height_shift_range=0.2,      # Random vertical shift\n",
        "    shear_range=0.15,            # Shearing transformations\n",
        "    zoom_range=0.2,              # Random zoom\n",
        "    horizontal_flip=True,        # Random horizontal flip\n",
        "    fill_mode='nearest'          # Filling in missing pixels\n",
        ")\n",
        "\n",
        "# Augment each image and save the augmented versions\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image file formats\n",
        "        img_path = os.path.join(data_dir, filename)\n",
        "        img = load_img(img_path)  # Load the image\n",
        "        img_array = img_to_array(img)  # Convert image to array\n",
        "        img_array = img_array.reshape((1,) + img_array.shape)  # Reshape to match input format for datagen\n",
        "\n",
        "        # Generate augmented images\n",
        "        prefix = os.path.splitext(filename)[0]  # Use original filename as prefix\n",
        "        i = 0\n",
        "        for batch in datagen.flow(\n",
        "            img_array,\n",
        "            batch_size=1,\n",
        "            save_to_dir=augmented_dir,\n",
        "            save_prefix=prefix,\n",
        "            save_format='jpg'\n",
        "        ):\n",
        "            i += 1\n",
        "            if i > 5:  # Generate 5 augmented images per input image\n",
        "                break\n",
        "\n",
        "print(f\"Data augmentation completed. Augmented images are saved in: {augmented_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cqYF_BhXf5D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_3OHBvnZHM2",
        "outputId": "84f52b35-725f-4a23-a510-d289423ccbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data augmentation completed. Augmented images are saved in: /content/augmented1/images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/hw_drawings/Dynamic Spiral Test\"  # Replace with the folder containing spiral images\n",
        "augmented_dir = \"/content/augmented1/images\"  # Replace with the folder to save augmented images\n",
        "\n",
        "# Create the augmented directory if it does not exist\n",
        "if not os.path.exists(augmented_dir):\n",
        "    os.makedirs(augmented_dir)\n",
        "\n",
        "# ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,           # Random rotation within 20 degrees\n",
        "    width_shift_range=0.2,       # Random horizontal shift\n",
        "    height_shift_range=0.2,      # Random vertical shift\n",
        "    shear_range=0.15,            # Shearing transformations\n",
        "    zoom_range=0.2,              # Random zoom\n",
        "    horizontal_flip=True,        # Random horizontal flip\n",
        "    fill_mode='nearest'          # Filling in missing pixels\n",
        ")\n",
        "\n",
        "# Augment each image and save the augmented versions\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image file formats\n",
        "        img_path = os.path.join(data_dir, filename)\n",
        "        img = load_img(img_path)  # Load the image\n",
        "        img_array = img_to_array(img)  # Convert image to array\n",
        "        img_array = img_array.reshape((1,) + img_array.shape)  # Reshape to match input format for datagen\n",
        "\n",
        "        # Generate augmented images\n",
        "        prefix = os.path.splitext(filename)[0]  # Use original filename as prefix\n",
        "        i = 0\n",
        "        for batch in datagen.flow(\n",
        "            img_array,\n",
        "            batch_size=1,\n",
        "            save_to_dir=augmented_dir,\n",
        "            save_prefix=prefix,\n",
        "            save_format='jpg'\n",
        "        ):\n",
        "            i += 1\n",
        "            if i > 5:  # Generate 5 augmented images per input image\n",
        "                break\n",
        "\n",
        "print(f\"Data augmentation completed. Augmented images are saved in: {augmented_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDdR-CSnZIxb",
        "outputId": "2d500007-700f-4d13-a687-91be6c985a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data augmentation completed. Augmented images are saved in: /content/augmented2/images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/hw_drawings/Static Spiral Test\"  # Replace with the folder containing spiral images\n",
        "augmented_dir = \"/content/augmented2/images\"  # Replace with the folder to save augmented images\n",
        "\n",
        "# Create the augmented directory if it does not exist\n",
        "if not os.path.exists(augmented_dir):\n",
        "    os.makedirs(augmented_dir)\n",
        "\n",
        "# ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,           # Random rotation within 20 degrees\n",
        "    width_shift_range=0.2,       # Random horizontal shift\n",
        "    height_shift_range=0.2,      # Random vertical shift\n",
        "    shear_range=0.15,            # Shearing transformations\n",
        "    zoom_range=0.2,              # Random zoom\n",
        "    horizontal_flip=True,        # Random horizontal flip\n",
        "    fill_mode='nearest'          # Filling in missing pixels\n",
        ")\n",
        "\n",
        "# Augment each image and save the augmented versions\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for valid image file formats\n",
        "        img_path = os.path.join(data_dir, filename)\n",
        "        img = load_img(img_path)  # Load the image\n",
        "        img_array = img_to_array(img)  # Convert image to array\n",
        "        img_array = img_array.reshape((1,) + img_array.shape)  # Reshape to match input format for datagen\n",
        "\n",
        "        # Generate augmented images\n",
        "        prefix = os.path.splitext(filename)[0]  # Use original filename as prefix\n",
        "        i = 0\n",
        "        for batch in datagen.flow(\n",
        "            img_array,\n",
        "            batch_size=1,\n",
        "            save_to_dir=augmented_dir,\n",
        "            save_prefix=prefix,\n",
        "            save_format='jpg'\n",
        "        ):\n",
        "            i += 1\n",
        "            if i > 5:  # Generate 5 augmented images per input image\n",
        "                break\n",
        "\n",
        "print(f\"Data augmentation completed. Augmented images are saved in: {augmented_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2o9VZh5Z7A1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "### *ğŸ“Œ Step 1: Install & Import Libraries*\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install opencv-python numpy pandas tensorflow scikit-learn matplotlib\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### *ğŸ“Œ Step 2: Load Dataset*\n",
        "\n",
        "# Define dataset paths\n",
        "image_folder_pwp = \"/content/PWP\"  # Parkinson's images\n",
        "image_folder_healthy = \"/content/Healthy\"  # Healthy images\n",
        "\n",
        "# Get image filenames\n",
        "pwp_images = [os.path.join(image_folder_pwp, f) for f in os.listdir(image_folder_pwp) if f.endswith('.png')]\n",
        "healthy_images = [os.path.join(image_folder_healthy, f) for f in os.listdir(image_folder_healthy) if f.endswith('.png')]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### *ğŸ“Œ Step 3: Image Preprocessing*\n",
        "# Image Preprocessing Function\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
        "    img = cv2.resize(img, target_size)  # Resize to 224x224\n",
        "    img = img / 255.0  # Normalize\n",
        "    return img\n",
        "\n",
        "# Load and preprocess images\n",
        "X_images = []\n",
        "y_labels = []\n",
        "\n",
        "for img_path in pwp_images:\n",
        "    X_images.append(preprocess_image(img_path))\n",
        "    y_labels.append(1)  # Parkinsonâ€™s label\n",
        "\n",
        "for img_path in healthy_images:\n",
        "    X_images.append(preprocess_image(img_path))\n",
        "    y_labels.append(0)  # Healthy label\n",
        "\n",
        "X_images = np.array(X_images).reshape(-1, 224, 224, 1)  # Reshape for CNN input\n",
        "y_labels = np.array(y_labels)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### *ğŸ“Œ Step 4: Split Dataset*\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_images, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### *ğŸ“Œ Step 5: Build CNN Model for Feature Extraction*\n",
        "\n",
        "# Define CNN Model\n",
        "input_layer = Input(shape=(224, 224, 1))\n",
        "\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(input_layer)\n",
        "x = MaxPooling2D((2,2))(x)\n",
        "x = BatchNormalization()\n",
        "\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2,2))(x)\n",
        "x = BatchNormalization()\n",
        "\n",
        "x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2,2))(x)\n",
        "x = BatchNormalization()\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "feature_output = Dense(64, activation='relu', name=\"feature_vector\")(x)  # Output Feature Vector\n",
        "\n",
        "# Define Model\n",
        "cnn_feature_extractor = Model(inputs=input_layer, outputs=feature_output)\n",
        "\n",
        "# Compile Model (No need for classification head)\n",
        "cnn_feature_extractor.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
        "cnn_feature_extractor.summary()\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### *ğŸ“Œ Step 6: Train CNN Model*\n",
        "\n",
        "# Train CNN Model for Feature Extraction\n",
        "history = cnn_feature_extractor.fit(\n",
        "    X_train, np.zeros((len(X_train), 64)),  # Dummy target since we're extracting features\n",
        "    validation_data=(X_test, np.zeros((len(X_test), 64))),\n",
        "    epochs=25,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "\n",
        "### *ğŸ“Œ Step 7: Extract Feature Vectors*\n",
        "\n",
        "# Extract Features from Images\n",
        "feature_vectors = cnn_feature_extractor.predict(X_images)\n",
        "\n",
        "# Save Feature Vectors for Multimodal Model\n",
        "np.save(\"handwriting_features.npy\", feature_vectors)\n",
        "np.save(\"handwriting_labels.npy\", y_labels)\n",
        "\n",
        "print(\"Feature vectors shape:\", feature_vectors.shape)  # Should be (num_samples, 64)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbIUAHNrsVet",
        "outputId": "99a6c877-5efa-40ef-a692-013c9aebf5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "   creating: dataset/spiral/\n",
            "   creating: dataset/spiral/testing/\n",
            "   creating: dataset/spiral/testing/healthy/\n",
            "  inflating: dataset/spiral/testing/healthy/V01HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V02HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V03HE1.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V04HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V05HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V06HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V07HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V08HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V09HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V10HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V11HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE12.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE13.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE14.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE15.png  \n",
            "   creating: dataset/spiral/testing/parkinson/\n",
            "  inflating: dataset/spiral/testing/parkinson/V01PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V02PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V03PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V03PE04.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V03PE07.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V04PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V05PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V06PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V07PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V08PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V09PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V10PE03.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V11PE03.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V14PE03.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V15PE01.png  \n",
            "   creating: dataset/spiral/training/\n",
            "   creating: dataset/spiral/training/healthy/\n",
            "  inflating: dataset/spiral/training/healthy/V01HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V01HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V02HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V02HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V03HE2.png  \n",
            "  inflating: dataset/spiral/training/healthy/V03HE3.png  \n",
            "  inflating: dataset/spiral/training/healthy/V04HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V04HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V05HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V05HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V06HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V06HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V07HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V07HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V08HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V08HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V09HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V09HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V10HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V10HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V11HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V11HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V12HE01.png  \n",
            "  inflating: dataset/spiral/training/healthy/V12HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V12HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE01.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE04.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE05.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE06.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE07.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE08.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE09.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE10.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE11.png  \n",
            "   creating: dataset/spiral/training/parkinson/\n",
            "  inflating: dataset/spiral/training/parkinson/V01PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V01PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V02PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V02PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE05.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE06.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE08.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE09.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V04PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V04PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V05PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V05PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V06PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V06PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V07PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V07PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V08PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V08PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V09PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V09PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V10PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V10PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V11PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V11PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V12PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V12PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V12PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V13PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V13PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V13PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V14PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V14PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V15PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V15PE03.png  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r7zPe4X7oQ_",
        "outputId": "41e1d410-db53-46fb-f634-10a3605761dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Features and labels saved to output_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths to the dataset\n",
        "dataset_path = '/content/dataset'\n",
        "spiral_path = os.path.join(dataset_path, 'spiral')\n",
        "training_path = os.path.join(spiral_path, 'training')\n",
        "healthy_training_path = os.path.join(training_path, 'healthy')\n",
        "parkinson_training_path = os.path.join(training_path, 'parkinson')\n",
        "\n",
        "# Helper function to load and preprocess images\n",
        "def load_and_preprocess_images(folder_path, target_size=(224, 224)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if file_name.lower().endswith(('png', 'jpg', 'jpeg', 'tiff')):\n",
        "            # Load the image in grayscale\n",
        "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image\n",
        "            img_resized = cv2.resize(img, target_size)\n",
        "            # Convert grayscale to RGB (required for ResNet50)\n",
        "            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
        "            # Normalize and preprocess for ResNet\n",
        "            img_preprocessed = preprocess_input(img_rgb)\n",
        "            images.append(img_preprocessed)\n",
        "            # Assign label: 1 for Parkinson's, 0 for healthy\n",
        "            label = 1 if 'parkinson' in folder_path.lower() else 0\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images and labels\n",
        "healthy_images, healthy_labels = load_and_preprocess_images(healthy_training_path)\n",
        "parkinson_images, parkinson_labels = load_and_preprocess_images(parkinson_training_path)\n",
        "\n",
        "# Combine data and labels\n",
        "X = np.concatenate((healthy_images, parkinson_images), axis=0)\n",
        "y = np.concatenate((healthy_labels, parkinson_labels), axis=0)\n",
        "\n",
        "# Shuffle the dataset\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pretrained ResNet50 without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add a global average pooling layer for feature extraction\n",
        "feature_extractor = tf.keras.Model(inputs=base_model.input, outputs=tf.keras.layers.GlobalAveragePooling2D()(base_model.output))\n",
        "\n",
        "# Extract features for the training and testing sets\n",
        "train_features = feature_extractor.predict(X_train, batch_size=32)\n",
        "test_features = feature_extractor.predict(X_test, batch_size=32)\n",
        "\n",
        "# Save features and labels\n",
        "output_folder = 'output_data'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "np.save(os.path.join(output_folder, 'train_features.npy'), train_features)\n",
        "np.save(os.path.join(output_folder, 'train_labels.npy'), y_train)\n",
        "np.save(os.path.join(output_folder, 'test_features.npy'), test_features)\n",
        "np.save(os.path.join(output_folder, 'test_labels.npy'), y_test)\n",
        "\n",
        "print(f\"Features and labels saved to {output_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTwMBhi29Iwy",
        "outputId": "a8e13957-e4c2-4930-b8a9-8341bb5f2e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r\u001b[1m1/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m7s\u001b[0m 8s/step"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x784ceca198a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Features and labels saved in human-readable format.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths to the dataset\n",
        "dataset_path = '/content/dataset'\n",
        "spiral_path = os.path.join(dataset_path, 'spiral')\n",
        "training_path = os.path.join(spiral_path, 'training')\n",
        "healthy_training_path = os.path.join(training_path, 'healthy')\n",
        "parkinson_training_path = os.path.join(training_path, 'parkinson')\n",
        "\n",
        "# Helper function to load and preprocess images\n",
        "def load_and_preprocess_images(folder_path, target_size=(224, 224)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if file_name.lower().endswith(('png', 'jpg', 'jpeg', 'tiff')):\n",
        "            # Load the image in grayscale\n",
        "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image\n",
        "            img_resized = cv2.resize(img, target_size)\n",
        "            # Convert grayscale to RGB (required for ResNet50)\n",
        "            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
        "            # Normalize and preprocess for ResNet\n",
        "            img_preprocessed = preprocess_input(img_rgb)\n",
        "            images.append(img_preprocessed)\n",
        "            # Assign label: 1 for Parkinson's, 0 for healthy\n",
        "            label = 1 if 'parkinson' in folder_path.lower() else 0\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images and labels\n",
        "healthy_images, healthy_labels = load_and_preprocess_images(healthy_training_path)\n",
        "parkinson_images, parkinson_labels = load_and_preprocess_images(parkinson_training_path)\n",
        "\n",
        "# Combine data and labels\n",
        "X = np.concatenate((healthy_images, parkinson_images), axis=0)\n",
        "y = np.concatenate((healthy_labels, parkinson_labels), axis=0)\n",
        "\n",
        "# Shuffle the dataset\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pretrained ResNet50 without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add a global average pooling layer for feature extraction\n",
        "feature_extractor = tf.keras.Model(inputs=base_model.input, outputs=tf.keras.layers.GlobalAveragePooling2D()(base_model.output))\n",
        "\n",
        "# Extract features for the training and testing sets\n",
        "train_features = feature_extractor.predict(X_train, batch_size=32)\n",
        "test_features = feature_extractor.predict(X_test, batch_size=32)\n",
        "\n",
        "# Save features and labels in a readable format\n",
        "output_folder = 'output_data1'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Save training features and labels\n",
        "with open(os.path.join(output_folder, 'train_features.txt'), 'w') as f:\n",
        "    for feature_vector in train_features:\n",
        "        f.write(', '.join(map(str, feature_vector)) + '\\n')\n",
        "\n",
        "with open(os.path.join(output_folder, 'train_labels.txt'), 'w') as f:\n",
        "    for label in y_train:\n",
        "        f.write(str(label) + '\\n')\n",
        "\n",
        "# Save testing features and labels\n",
        "with open(os.path.join(output_folder, 'test_features.txt'), 'w') as f:\n",
        "    for feature_vector in test_features:\n",
        "        f.write(', '.join(map(str, feature_vector)) + '\\n')\n",
        "\n",
        "with open(os.path.join(output_folder, 'test_labels.txt'), 'w') as f:\n",
        "    for label in y_test:\n",
        "        f.write(str(label) + '\\n')\n",
        "\n",
        "print(\"Features and labels saved in human-readable format.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ4Wuw3ZBszz",
        "outputId": "c70b0259-324c-49fb-e266-f872deb7136e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "   creating: dataset/spiral/\n",
            "   creating: dataset/spiral/testing/\n",
            "   creating: dataset/spiral/testing/healthy/\n",
            "  inflating: dataset/spiral/testing/healthy/V01HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V02HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V03HE1.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V04HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V05HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V06HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V07HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V08HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V09HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V10HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V11HE01.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE12.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE13.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE14.png  \n",
            "  inflating: dataset/spiral/testing/healthy/V55HE15.png  \n",
            "   creating: dataset/spiral/testing/parkinson/\n",
            "  inflating: dataset/spiral/testing/parkinson/V01PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V02PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V03PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V03PE04.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V03PE07.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V04PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V05PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V06PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V07PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V08PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V09PE01.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V10PE03.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V11PE03.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V14PE03.png  \n",
            "  inflating: dataset/spiral/testing/parkinson/V15PE01.png  \n",
            "   creating: dataset/spiral/training/\n",
            "   creating: dataset/spiral/training/healthy/\n",
            "  inflating: dataset/spiral/training/healthy/V01HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V01HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V02HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V02HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V03HE2.png  \n",
            "  inflating: dataset/spiral/training/healthy/V03HE3.png  \n",
            "  inflating: dataset/spiral/training/healthy/V04HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V04HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V05HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V05HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V06HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V06HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V07HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V07HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V08HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V08HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V09HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V09HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V10HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V10HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V11HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V11HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V12HE01.png  \n",
            "  inflating: dataset/spiral/training/healthy/V12HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V12HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE01.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE02.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE03.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE04.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE05.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE06.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE07.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE08.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE09.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE10.png  \n",
            "  inflating: dataset/spiral/training/healthy/V55HE11.png  \n",
            "   creating: dataset/spiral/training/parkinson/\n",
            "  inflating: dataset/spiral/training/parkinson/V01PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V01PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V02PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V02PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE05.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE06.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE08.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V03PE09.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V04PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V04PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V05PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V05PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V06PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V06PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V07PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V07PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V08PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V08PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V09PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V09PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V10PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V10PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V11PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V11PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V12PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V12PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V12PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V13PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V13PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V13PE03.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V14PE01.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V14PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V15PE02.png  \n",
            "  inflating: dataset/spiral/training/parkinson/V15PE03.png  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1rkjl4L9_-c",
        "outputId": "6303f866-22aa-486c-9968-1d7fc6a3fdfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 10s/step - accuracy: 0.4148 - loss: 1.3689 - val_accuracy: 0.6000 - val_loss: 0.8825\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7s/step - accuracy: 0.4525 - loss: 1.0762 - val_accuracy: 0.4000 - val_loss: 1.0637\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7s/step - accuracy: 0.5826 - loss: 0.7877 - val_accuracy: 0.4667 - val_loss: 0.9067\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8s/step - accuracy: 0.6424 - loss: 0.6440 - val_accuracy: 0.8000 - val_loss: 0.4514\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9s/step - accuracy: 0.7399 - loss: 0.5032 - val_accuracy: 0.8000 - val_loss: 0.4511\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7s/step - accuracy: 0.5747 - loss: 0.5711 - val_accuracy: 0.8667 - val_loss: 0.3692\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9s/step - accuracy: 0.8765 - loss: 0.4144 - val_accuracy: 0.8000 - val_loss: 0.3823\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 6s/step - accuracy: 0.8101 - loss: 0.3842 - val_accuracy: 0.8000 - val_loss: 0.4426\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8s/step - accuracy: 0.8076 - loss: 0.3738 - val_accuracy: 0.8000 - val_loss: 0.4097\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8s/step - accuracy: 0.8310 - loss: 0.3376 - val_accuracy: 0.8000 - val_loss: 0.3018\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3s/step - accuracy: 0.8681 - loss: 0.2977\n",
            "Test Accuracy: 0.86\n",
            "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4s/step\n",
            "Features and labels saved in a single .npy format.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths to the dataset\n",
        "dataset_path = '/content/dataset'\n",
        "spiral_path = os.path.join(dataset_path, 'spiral')\n",
        "training_path = os.path.join(spiral_path, 'training')\n",
        "healthy_training_path = os.path.join(training_path, 'healthy')\n",
        "parkinson_training_path = os.path.join(training_path, 'parkinson')\n",
        "\n",
        "# Helper function to load and preprocess images\n",
        "def load_and_preprocess_images(folder_path, target_size=(224, 224)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if file_name.lower().endswith(('png', 'jpg', 'jpeg', 'tiff')):\n",
        "            # Load the image in grayscale\n",
        "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image\n",
        "            img_resized = cv2.resize(img, target_size)\n",
        "            # Convert grayscale to RGB (required for ResNet50)\n",
        "            img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB)\n",
        "            # Normalize and preprocess for ResNet\n",
        "            img_preprocessed = preprocess_input(img_rgb)\n",
        "            images.append(img_preprocessed)\n",
        "            # Assign label: 1 for Parkinson's, 0 for healthy\n",
        "            label = 1 if 'parkinson' in folder_path.lower() else 0\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images and labels\n",
        "healthy_images, healthy_labels = load_and_preprocess_images(healthy_training_path)\n",
        "parkinson_images, parkinson_labels = load_and_preprocess_images(parkinson_training_path)\n",
        "\n",
        "# Combine data and labels\n",
        "X = np.concatenate((healthy_images, parkinson_images), axis=0)\n",
        "y = np.concatenate((healthy_labels, parkinson_labels), axis=0)\n",
        "\n",
        "# Shuffle the dataset\n",
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Load pretrained ResNet50 and add custom layers\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)  # Binary classification (0 or 1)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the base model's layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X, y)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Extract features using the trained model\n",
        "feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-3].output)\n",
        "features = feature_extractor.predict(X, batch_size=32)\n",
        "\n",
        "# Save features and labels as a single .npy file\n",
        "output_folder = 'output_data2'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Save combined dataset\n",
        "np.save(os.path.join(output_folder, 'features.npy'), features)\n",
        "np.save(os.path.join(output_folder, 'labels.npy'), y)\n",
        "\n",
        "print(\"Features and labels saved in a single .npy format.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
