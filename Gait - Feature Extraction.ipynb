{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmHzYDKI4kpO",
        "outputId": "e7a2c5c4-da17-4685-b272-4b577394329a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBNTeO565RF9",
        "outputId": "8fdd08ae-ac46-43c7-8b6c-cc468d347e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 56 dataset files.\n"
          ]
        }
      ],
      "source": [
        "import glob # Import the glob module\n",
        "# Define dataset path\n",
        "dataset_path = \"/content/drive/MyDrive/Filtered Data\"  # Update this path\n",
        "\n",
        "# Get all text files inside 12 subfolders\n",
        "txt_files = glob.glob(os.path.join(dataset_path, \"**/*.txt\"), recursive=True)\n",
        "\n",
        "print(f\"Found {len(txt_files)} dataset files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkDSu2dC7e1r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your dataset folder in Google Drive\n",
        "base_path = \"/content/drive/MyDrive/Filtered Data\"\n",
        "\n",
        "# List to store file paths\n",
        "txt_files = []\n",
        "\n",
        "# Traverse the dataset folders\n",
        "for folder in sorted(os.listdir(base_path)):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "\n",
        "    if os.path.isdir(folder_path):  # Ensure it's a folder\n",
        "        if folder == \"008\":  # Special handling for folder 8\n",
        "            for subfolder in [\"OFF_1\", \"OFF_2\"]:\n",
        "                subfolder_path = os.path.join(folder_path, subfolder)\n",
        "                if os.path.exists(subfolder_path):  # Check if subfolder exists\n",
        "                    for file in os.listdir(subfolder_path):\n",
        "                        if file.endswith(\".txt\"):\n",
        "                            txt_files.append(os.path.join(subfolder_path, file))\n",
        "        else:  # All other folders where .txt files are directly present\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.endswith(\".txt\"):\n",
        "                    txt_files.append(os.path.join(folder_path, file))\n",
        "\n",
        "# Verify collected files\n",
        "print(f\"Total .txt files found: {len(txt_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3n_g185DpKv",
        "outputId": "f9dee868-fc08-4bbf-8f64-0381f3320fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total .txt files found: 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYeRBZ627aR3",
        "outputId": "e14067b5-eeac-4674-9f0a-419f71c5dade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gait Data Shape: (6211056, 28)\n"
          ]
        }
      ],
      "source": [
        "# Columns related to acceleration & gyroscope (32-59)\n",
        "gait_columns = list(range(32, 59))  # Includes x, y, z accelerometer + gyro\n",
        "label_column = [60]  # Freezing of Gait (FoG) Label\n",
        "\n",
        "# Initialize list for storing processed data\n",
        "all_data = []\n",
        "\n",
        "# Read all files\n",
        "for file in txt_files:\n",
        "    df = pd.read_csv(file, header=None)  # No header in dataset\n",
        "    gait_data = df[gait_columns + label_column]  # Extract only gait & labels\n",
        "    all_data.append(gait_data)\n",
        "\n",
        "# Merge all files\n",
        "gait_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "print(\"Gait Data Shape:\", gait_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Ensure GPU usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert gait data to PyTorch tensor and move to GPU\n",
        "gait_tensor = torch.tensor(gait_df.values, dtype=torch.float32, device=device)\n",
        "\n",
        "window_size = 1000  # 2 sec (500Hz)\n",
        "step_size = 500     # 50% overlap\n",
        "\n",
        "# Batch-wise segmentation for speed\n",
        "num_samples = (gait_tensor.shape[0] - window_size) // step_size\n",
        "\n",
        "# Preallocate tensors\n",
        "X = torch.zeros((num_samples, window_size, gait_tensor.shape[1] - 1), device=device)\n",
        "y = torch.zeros((num_samples,), dtype=torch.long, device=device)\n",
        "\n",
        "# Fill tensors\n",
        "for i in range(num_samples):\n",
        "    start = i * step_size\n",
        "    X[i] = gait_tensor[start : start + window_size, :-1]  # All gait features\n",
        "    y[i] = gait_tensor[start + window_size - 1, -1]  # Last label in window\n",
        "\n",
        "print(\"Segmented Data Shape:\", X.shape, \"Labels Shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVQh67OJXXtd",
        "outputId": "ea6f9888-6578-45b9-8e4b-cd61a22f69a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Data Shape: torch.Size([12420, 1000, 27]) Labels Shape: torch.Size([12420])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Move data to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert DataFrame to NumPy\n",
        "gait_data = gait_df.iloc[:, :-1].values  # Features\n",
        "labels = gait_df.iloc[:, -1].values  # Labels\n",
        "\n",
        "# Convert to PyTorch tensors and move to GPU\n",
        "gait_tensor = torch.tensor(gait_data, dtype=torch.float32, device=device)\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.int64, device=device)\n",
        "\n",
        "# Segmentation parameters\n",
        "window_size = 1000  # 2 seconds\n",
        "step_size = 500\n",
        "\n",
        "segments = []\n",
        "labels = []\n",
        "\n",
        "# GPU batch segmentation\n",
        "for i in range(0, len(gait_tensor) - window_size, step_size):\n",
        "    segment = gait_tensor[i:i+window_size]  # Feature segment\n",
        "    label = labels_tensor[i+window_size-1]  # Label at window end\n",
        "    segments.append(segment)\n",
        "    labels.append(label)\n",
        "\n",
        "# Stack as tensors\n",
        "X = torch.stack(segments).to(device)\n",
        "y = torch.tensor(labels, dtype=torch.int64, device=device)\n",
        "\n",
        "print(\"Segmented Data Shape:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHWUfZGnYFT0",
        "outputId": "c7b0ca65-c66e-4969-ffbf-222287499474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmented Data Shape: torch.Size([12421, 1000, 27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "\n",
        "        # CNN Layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # LSTM Layers\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected output layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Convert to (batch, channels, time) for CNN\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # CNN Feature Extraction\n",
        "        x = x.permute(0, 2, 1)  # Back to (batch, time, features)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_features = lstm_out[:, -1, :]  # Extract last LSTM hidden state\n",
        "\n",
        "        output = self.fc(lstm_features)  # Final classification output\n",
        "        return output\n",
        "\n",
        "    # New method to extract feature vectors\n",
        "    def extract_features(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_features = lstm_out[:, -1, :]  # Extract feature vector before final FC layer\n",
        "        return lstm_features\n"
      ],
      "metadata": {
        "id": "_sru3F6-Y7m4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define model parameters\n",
        "input_size = 27  # Number of gait features\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1  # Binary classification (FoG or no FoG)\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_LSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "\n",
        "# Define Loss and Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X).squeeze()\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "print(\"CNN-LSTM Training Completed ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXa7aUEYZOhP",
        "outputId": "255f1b2c-a3ba-4d18-b92f-d76bfa6026df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.575844870790408\n",
            "Epoch 2, Loss: 0.5622298310232315\n",
            "Epoch 3, Loss: 0.5670619021466308\n",
            "Epoch 4, Loss: 0.5626194989757906\n",
            "Epoch 5, Loss: 0.5573569317722628\n",
            "CNN-LSTM Training Completed ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X).squeeze()\n",
        "        preds = torch.round(torch.sigmoid(outputs))  # Convert logits to binary predictions\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}% ✅\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1w5PP4dZvlX",
        "outputId": "c1ea8324-d65f-42af-9259-4f0d00a8b19f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 77.79% ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "feature_vectors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, _ in test_loader:  # No need for labels here\n",
        "        batch_X = batch_X.to(device)\n",
        "        features = model.extract_features(batch_X)  # Extract features from LSTM layer\n",
        "        feature_vectors.append(features.cpu().numpy())\n",
        "\n",
        "# Convert to numpy array\n",
        "feature_vectors = np.vstack(feature_vectors)\n",
        "\n",
        "# Save feature vectors to a file\n",
        "np.save('gait_feature_vectors.npy', feature_vectors)\n",
        "print(\"Feature vectors saved successfully! ✅\")\n"
      ],
      "metadata": {
        "id": "5Ds6HiLObENV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32eaf2c5-1743-4e69-aa4f-2ba6a7a25407"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature vectors saved successfully! ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('gait_feature_vectors.npy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XBRh7ATreksD",
        "outputId": "0eb2ceba-5581-4f2a-dd74-8aa8601d406f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b3b91fe3-888a-4aa7-b8a7-d87c9a05bdd9\", \"gait_feature_vectors.npy\", 1272448)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature_vectors.shape)  # Should be (num_samples, feature_dim)\n",
        "print(\"Example feature vector:\", feature_vectors[0][:10])  # Print first 10 values of first vector\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBAf8HASjMee",
        "outputId": "32444d38-da39-4887-81ae-59a7c24dc773"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2485, 128)\n",
            "Example feature vector: [-0.03943942 -0.2911995   0.2068045  -0.04807587  0.02643567  0.2355742\n",
            "  0.0088343   0.03526265 -0.00297631  0.07361782]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}